{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kushagra Jalota\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Kushagra Jalota\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "submit_loc = r\"C:\\Users\\Kushagra Jalota\\sub.csv\"\n",
    "\n",
    "train_loc = r\"C:\\Users\\Kushagra Jalota\\Loan-Prediction-Classification-master\\claim_trainnew.csv\"\n",
    "test_loc = r\"C:\\Users\\Kushagra Jalota\\Loan-Prediction-Classification-master\\claim_testnew.csv\"\n",
    "\n",
    "train = pd.read_csv(train_loc)\n",
    "test = pd.read_csv(test_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create dummy variables then shuffle the train set.\n",
    "\n",
    "cols = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n",
    "       'Credit_History', 'Property_Area']\n",
    "\n",
    "train = pd.get_dummies(train, columns = cols)\n",
    "test = pd.get_dummies(test, columns = cols)\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Loan_ID', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n",
       "       'Loan_Amount_Term', 'Loan_Status', 'Gender_Female', 'Gender_Male',\n",
       "       'Gender_Unknown', 'Married_No', 'Married_Unknown', 'Married_Yes',\n",
       "       'Dependents_0', 'Dependents_1', 'Dependents_2', 'Dependents_3',\n",
       "       'Dependents_Unknown', 'Education_Graduate', 'Education_Not Graduate',\n",
       "       'Self_Employed_No', 'Self_Employed_Unknown', 'Self_Employed_Yes',\n",
       "       'Credit_History_No', 'Credit_History_Unknown', 'Credit_History_Yes',\n",
       "       'Property_Area_Rural', 'Property_Area_Semiurban',\n",
       "       'Property_Area_Urban'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set the target variable.\n",
    "\n",
    "y = train.Loan_Status\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Drop the unnecessary columns and prepare the data for our submission file.\n",
    "\n",
    "train =  train.drop(['Loan_ID', 'Loan_Status'],axis=1)\n",
    "X = train.astype(np.float32)\n",
    "submit = test['Loan_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do the same for the test set. Unlike the train set there was no unknown data in the married \n",
    "#column on the test set. As a result we have to insert a Married Unknown column as Pandas did\n",
    "#not create this column when we ran pd.get_dummies() on the test set.\n",
    "\n",
    "test = test.drop('Loan_ID', axis=1)\n",
    "test.insert(8, 'Married_Unknown', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use Scikit-Learn's train_test_split function to create train and validation sets.\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function to run a gradient boosted classifier over our data. The reason we have to\n",
    "#do this is due to the way Python uses parallelization on Windows.\n",
    "\n",
    "\n",
    "#Note numerous different values were used in the param_grid to hone in on the best paramater\n",
    "#combinations. The param grid below is the final one I used.\n",
    "\n",
    "def model(X_train, X_val, y_train, y_val):\n",
    "    if __name__ == '__main__':\n",
    "    \n",
    "        param_grid = {'learning_rate': [0.03, 0.035],\n",
    "                      'max_depth': [3, 4, 5],\n",
    "                      'min_samples_leaf': [17, 18],\n",
    "                      'max_features': [1.0, 0.95, 0.9],\n",
    "                      'n_estimators': [100, 300, 500]\n",
    "                      }\n",
    "\n",
    "        estimator = GridSearchCV(estimator=GradientBoostingClassifier(),\n",
    "                                 param_grid=param_grid,\n",
    "                                 n_jobs=-1)\n",
    "\n",
    "        estimator.fit(X_train, y_train)\n",
    "\n",
    "        best_params = estimator.best_params_\n",
    "                                 \n",
    "        validation_accuracy = estimator.score(X_val, y_val)\n",
    "        print('Validation accuracy: ', validation_accuracy)\n",
    "        \n",
    "        return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params that appeared most often after running the model ten times.\n",
    "    \n",
    "params = {'min_samples_leaf': 17, 'max_features': 0.95, 'max_depth': 3,\n",
    "          'learning_rate': 0.03, 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.03, loss='deviance', max_depth=3,\n",
       "              max_features=0.95, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=17,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=500, presort='auto', random_state=None,\n",
       "              subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model using our data and the best parameters found by GridSearchCV.\n",
    "\n",
    "model = GradientBoostingClassifier(**params)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make predictions on the test set.\n",
    "\n",
    "preds = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create submission file.\n",
    "\n",
    "preds = pd.Series(preds)\n",
    "submit = pd.concat([submit, preds], names=['Loan_ID', 'Loan_Status'], axis=1)\n",
    "submit.columns = ['Loan_ID', 'Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create CSV file for submission.\n",
    "\n",
    "submit.to_csv('finalsub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here's how the model performed on the leaderboard.\n",
    "#score 0.798611, position 33/413. \n",
    "\n",
    "#Note - model submitted on 09/03/2017."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
